setting:
  output_dir: "/home/jinpeng/blob/vigstandard_data/v-jinpewang/experiments/VideoGPT4" # experiments/VideoGPT4/
  src_dir: "./src"
  add_time_stamp: True
  resume_from_checkpoint: True
  prompt_template_name: "videogpt4"

data_setting:
  local_prefix: "/home/jinpeng/blob/vigstandard_data/v-jinpewang"
  train:
    img_txt_path: >
      /dataset/laion400m_wds_w_gen_caption/{00000..00001}.tar;
      /dataset/data_comp_1b/shards_full/{0002303..0002303}.tar
    inter_img_txt_path: >
      /dataset/obelics_wds_w_score_w_gen_caption/chunk0/{000000000..000000002}.tar;
      /dataset/mmc4/core_ff_w_clean_w_rule1_chunk_wds_w_gen_caption/chunk1/{000000001..000000002}.tar;
      /dataset/cc3m_interlevel/{00000..00001}.tar
    vid_txt_wds_path: >
      /dataset/webvid2_5m_w_gen_caption/val/{000000000..000000001}.tar
    vid_txt_tsv_path: >
      /dataset/webvid2_5m/val_annotations_w_gen/val_w_gen_combined_224.tsv
    inter_vid_txt_wds_path: >

    inter_vid_txt_tsv_path: >

    instr_path:
      ""
  eval:
    img_txt_path: >
      /dataset/cc3m_wds_w_gen_caption/train/{00000..00001}.tar
    inter_img_txt_path: >
      /dataset/cc3m_interlevel/{00092..00092}.tar
    vid_txt_wds_path: >

    vid_txt_tsv_path: >

    inter_vid_txt_wds_path: >

    inter_vid_txt_tsv_path: >

    instr_path:
      ""


dataset_params:
  use_azfuse: False # !!!!!
  split_data_by_node: False # !!!!!
  upload_model_to_blob: True # !!!!!
  sampling_strategy: "round_robin" # round_robin, min, max,
  img_txt:
    use_img_txt: True
    clean_data_use_strategy: "noisy_only" # clean_only, clean_first, clean_last, clean_random, noisy_only, mixed: 50% clean, 50% noisy    
    tar_pre_cache: True # if true, download all tar files to local disk before training. Otherwise, download tar files on the fly
    pre_cache_ratio: 1.0 # if tar_pre_cache is True, only download 10% of tar files
  inter_img_txt:
    use_inter_img_txt: True
    MIN_KB: 10 # reject image smaller than 10KB
    MAX_IMAGE_PIXELS: 1000000000
    MAX_NUM_TOKENS_MMC4: 128  # max length of text token, 256 in flamingo (with larger memory)
    MAX_NUM_TOKENS_CC3M: 128  # max length of text token, 256 in flamingo (with larger memory)
    MAX_NUM_TOKENS_OBELICS: 128  # max length of text token, 256 in flamingo (with larger memory)
    MAX_NUM_IMAGES_MMC4: 3  # images num in each interlevel image_text pairs, 5 in flamingo
    MAX_NUM_IMAGES_CC3M: 3
    MAX_NUM_IMAGES_OBELICS: 3  # 50%+ obelics only have 1 images
    SIM_THRESHOLD_MMC4: 0.24
    SIM_THRESHOLD_OBELICS: 0.10
    TINY_IMAGE_SIZE_THRESHOLD: 1
    N_CHANNELS: 3
    INTERLEAVED_IMAGE_SIZE: 224
    clean_data_use_strategy: "low_simlarity" # clean_only, clean_first, clean_last, clean_random, noisy_only, mixed: 50% clean, 50% noisy
    interlevel_text_coherence: True # If True, sample MAX_NUM_TOKENS token from adjacent text; If False, sample K images and their matched text
    balanced_sampling: False # If true, sample K images and K texts possibly
    tar_pre_cache: True # if true, download all tar files to local disk before training. Otherwise, download tar files on the fly
    pre_cache_ratio: 1.0 # if tar_pre_cache is True, only download 10% of tar files
  vid_txt:
    use_vid_txt: True
    VIDEO_IMAGE_SIZE: 224
    VIDEO_FRAMES: 4
    MAX_SAMPLES: 300 # 500000, use 0.5M video-text pair at most, -1 for use all dataset
    clean_data_use_strategy: "clean_only" # clean_only, clean_first, clean_last, clean_random, noisy_only, mixed: 50% clean, 50% noisy
    read_mode: "wds" # tsv, wds  !!!!!     wds is memory consuming
    tar_pre_cache: True # if true, download all tar files to local disk before training. Otherwise, download tar files on the fly
    pre_cache_ratio: 1.0 # if tar_pre_cache is True, only download 10% of tar files
  inter_vid_txt:
    use_inter_vid_txt: True
    VIDEO_IMAGE_SIZE: 224
    VIDEO_SAMPLED_CLIPS: 2
    VIDEO_FRAMES: 3
    MAX_SAMPLES: 100 # 500000, use 0.5M video-text pair at most, -1 for use all dataset
    MAX_NUM_TOKENS: 128  # max length of text token, 256 in flamingo (with larger memory)
    clean_data_use_strategy: "clean_only" # clean_only, clean_first, clean_last, clean_random, noisy_only, mixed: 50% clean, 50% noisy
    read_mode: "wds" # tsv, wds  tsv downloaded from blob cache
    tar_pre_cache: True # if true, download all tar files to local disk before training. Otherwise, download tar files on the fly
    pre_cache_ratio: 0.5 # if tar_pre_cache is True, only download 10% of tar files

model_params:
  architecture: "cosmo" # cosmo, fuyu
  vision_encoder:
    vision_encoder_name: "clip" # clip/sam/sparseformer
    vision_encoder_arch: "ViT-L-14"
    vision_encoder_pretrained: "openai"
    tuning: False
    ckpt_path: ""
    cache_dir: "/datadrive_d/jinpeng/Code/cosmo/pretrained_models"
    custom_augment: True
  lang_model:
    name: "opt-iml-max-1.3b"
    lang_model_path: "/datadrive_d/jinpeng/Code/cosmo/pretrained_models/opt-iml-max-1.3b"
    tokenizer_path: "/datadrive_d/jinpeng/Code/cosmo/pretrained_models/opt-iml-max-1.3b"
    dim: 512
    num_tokens: 512
    unimodal_depth: 12
    multimodal_depth: 12
    interval_layer: 4
    use_memory_layer: False # if True, add 1 memory layer after unimodal layer, but require faiss library
  multimodality_model:
    latent_dim: 512 # latent space for contrastive learning
    use_contrastive_loss: True
    contrastive_temperature: 0.07
    contrastive_loss_weight: 0.5
    contrastive_gather_way: "single_gpu" # "single_node", "all_nodes", "single_gpu"
    cross_attention_compress_ratio: 1 # compress self-attention and feedforward layer in CrossAttention module
    only_attend_immediate_media: False # if True, only attend to immediate media in CrossAttention module
    qv_norm: True # if True, normalize q and v in CrossAttention module
  offline: False
  gradient_checkpointing: False

# 'adamw_hf', 'adamw_torch', 'adamw_torch_fused', 'adamw_torch_xla', 'adamw_apex_fused', 'adafactor', 'adamw_bnb_8bit', 'adamw_anyprecision', 'sgd', 'adagrad'
training_params:
  float_type: "fp16" # fp16, fp32, bf16
  optim: "adamw_hf"
  micro_batch_size: 8
  workers: 4
  num_epochs: 10
  lr_scheduler_type: "cosine" # cosine, linear, constant,  polynomial, cosine_with_restarts, inverse_sqrt
  # linear: from learning_rate to 0, constant: keep learning_rate, polynomial: from learning_rate to 0, cosine: from learning_rate to 0 
  learning_rate: 0.0003
  cutoff_len: 256
  warmup_steps: 10
  warmup_ratio: 0.01
  logging_steps: 10 # do not set it too small, otherwise it will slow down the training (due to the overhead of logging)
  save_steps: 50 # 1000 in general
  save_total_limit: 3
  # the following for validation
  eval_steps: 100
  eval: True # if evaluate
  max_eval_batches: 20
  data_weights:
    img_txt: 1
    vid_txt: 1
    inter_img_txt: 1
    inter_vid_txt: 1
  exception_handling: False # if True, the training will not stop when exception occurs
  ignore_data_skip: True # if False, when resume from checkpoint, need to iteration all dataloader until reach the last iteration (exception)
  data_resampling: False
  custom_dist_init_group_timeout: 6000 # 6000s = 100min

lora_params:
  lora: False
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "v_proj"]

wandb_params:
  wandb: False # True for use, False for not use
  wandb_project: "videoflamingo"
  wandb_run_name: "18m baseline"
  wandb_watch: "all"
  wandb_log_model: ""
  wandb_dir: "./experiments"
